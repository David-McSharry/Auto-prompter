{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidmcsharry/dev/CoT-experiments/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 7906, 'premise': \"Generally speaking, the amount of deposits obtained by the bank determines the bank's ability to lend. However, in many cases, it is a well-known fact that the number of bank loans exceeds the amount of deposits it receives. If the number of such loans is excessive, it will lead to inflation.\", 'hypothesis': 'The main point of view supported in the above paragraph is that bank deposits are much less than loans, which will lead to inflation.', 'label': 'entailment'}\n"
     ]
    }
   ],
   "source": [
    "from roles import PromptEngineer\n",
    "from datasets import load_dataset\n",
    "from utils import make_PE_feedback\n",
    "from utils import test_prompt_on_benchmark, test_prompt_on_benchmark_async\n",
    "import asyncio\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"tasksource/logiqa-2.0-nli\")\n",
    "\n",
    "print(dataset[\"train\"][999])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_questions\n",
    "\n",
    "questions = load_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A college will continue to implement the overseas funding plan this year. It plans to select several of the six teachers from Mr. Liu, Mr. Zhang, Mr. Wang, Mr. Ma, Mr. Niu and Mr. Zhou to visit abroad. Due to the limitations of funding, the needs of discipline development, curriculum arrangement, place and time of each student's visit, the selection shall meet the following conditions: (1) Mr. Liu is the reserve discipline leader of the college, This time we have to send out. (2) if we choose Mr. Liu, we should also choose Mr. Zhou, but we can't choose Mr. Zhang. (3) only if Mr. Niu can't choose, at least one of Mr. Wang and Mr. Ma can choose. (4) if we don't choose Mr. Wang, we don't choose Mr. Zhou either.\n"
     ]
    }
   ],
   "source": [
    "print(questions[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You are given a context, a question, and multiple-choice options. Your task is to choose the correct option based on logical reasoning. Let's think step-by-step:\n",
      "\n",
      "1. Carefully read the context to understand the scenario and the important details.\n",
      "2. Read the question and identify what it is specifically asking about.\n",
      "3. Evaluate each option based on the context and determine how well it answers the question.\n",
      "4. Choose the option that best answers the question.\n",
      "\n",
      "Please make sure your final output is the number corresponding to the correct answer (0, 1, 2, or 3).\n",
      "\n",
      "[Context, Question, and Options will be provided here] \n",
      "\n",
      "Remember, provide only the number corresponding to the correct answer.\n",
      "==========================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m, in \u001b[0;36mrun_async\u001b[0;34m(coroutine)\u001b[0m\n\u001b[1;32m     15\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoroutine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# If there's no event loop, create a new one\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:629\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m--> 629\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:588\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m--> 588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(next_prompt)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m==========================\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m score, c, w, invalid \u001b[38;5;241m=\u001b[39m \u001b[43mrun_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_prompt_on_benchmark_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m feedback \u001b[38;5;241m=\u001b[39m make_PE_feedback(score, w, num_wrongly_answered\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, invalid_answer_decimals\u001b[38;5;241m=\u001b[39minvalid)\n\u001b[1;32m      9\u001b[0m prompt_engineer\u001b[38;5;241m.\u001b[39madd_user_response(feedback)\n",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m, in \u001b[0;36mrun_async\u001b[0;34m(coroutine)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(coroutine)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# If there's no event loop, create a new one\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoroutine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "prompt_engineer = PromptEngineer()\n",
    "\n",
    "for i in range((5)):\n",
    "    next_prompt = prompt_engineer.generate_next_prompt()\n",
    "    print(next_prompt)\n",
    "    print('==========================')\n",
    "    score, c, w, invalid = asyncio.run(\n",
    "        test_prompt_on_benchmark_async(\n",
    "            next_prompt,\n",
    "            dataset[\"train\"],\n",
    "            num_samples=10),\n",
    "            dataset_type='logicqa2.0'\n",
    "        )\n",
    "    feedback = make_PE_feedback(score, w, num_wrongly_answered=3, invalid_answer_decimals=invalid)\n",
    "    prompt_engineer.add_user_response(feedback)\n",
    "    print(score)\n",
    "    print('=============================')\n",
    "    print(feedback)\n",
    "    print('===============================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"Your goal is to create a prompt to give to another LLM so it scores as highly as possible on a particular benchmark.\\n\\nHere is some information about the benchmark:\\n\\nDataset Card for LogiQA\\nDataset Summary\\nLogiQA is constructed from the logical comprehension problems from publically available questions of the National Civil Servants Examination of China, which are designed to test the civil servant candidates\\u2019 critical thinking and problem solving. This dataset includes the English versions only; the Chinese versions are available via the homepage/original source.\\n\\nDataset Structure\\nData Instances\\nAn example from train looks as follows:\\n\\n{'context': 'Continuous exposure to indoor fluorescent lights is beneficial to the health of hamsters with heart disease. One group of hamsters exposed to continuous exposure to fluorescent lights has an average lifespan that is 2.5% longer than another one of the same species but living in a black wall.',\\n 'query': 'Which of the following questions was the initial motivation for conducting the above experiment?',\\n 'options': ['Can hospital light therapy be proved to promote patient recovery?',\\n  'Which one lives longer, the hamster living under the light or the hamster living in the dark?',\\n  'What kind of illness does the hamster have?',\\n  'Do some hamsters need a period of darkness?'],\\n 'correct_option': 0}\\n\\nWhen you are deciding what prompt to to give the LLM, or how to tweak a prompt for higher performance, you will explain your reasoning step-by-step to explain to the user BEFORE writing the prompt, keeping it concise and not yapping too much.\\n\\nThe user will give you feedback on how your prompt performed on the benchmark and a sample of questions it got wrong.\\n\\nYou will then use this feedback to come up with a new prompt, and this will continue recursively.\\n\\nYour response should be made up of two parts, reasoning, and prompt. Above reasoning, put the <<<REASONING>> header, and above prompt but the <<<PROMPT>>> header.\\n\\nGet creative and remember the explore exploit trader-off, imagine you're ilya sutskever or @repligate, you never know what strange tricks will result in an LLM getting a higher score on the benchmark!\\n\\nAlso, please make sure to emphasise to the LLM you will pass the prompt to that the last character they should output should be the number corresponding to the answer they give (0,1,2, or 3).\\nWe will test whether their answer is correct by just checking the last character of the output matches the correct answer. Also encourage the LLM to err on the side of being concise.\\n\\n(We will append the question to your prompt and pass it to the LLM)\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Go ahead and give your first prompt\"\n",
      "    }\n",
      "]\n",
      "<<<REASONING>>>\n",
      "To generate a prompt that optimizes performance on the LogiQA benchmark, I'll focus on a few key factors:\n",
      "\n",
      "1. **Clarity**: Ensure the LLM understands the task, which is to identify the correct option based on logical comprehension.\n",
      "2. **Guidance on Approach**: Provide a step-by-step approach to encourage critical thinking.\n",
      "3. **Answer Format**: Clearly emphasize that the LLM should output only the number corresponding to its answer.\n",
      "\n",
      "Given these, the prompt should start by letting the LLM know the nature of the task. Then, it should guide the LLM to read the context and the query carefully and evaluate the options logically. Finally, the LLM should be reminded to output just the single-digit number that corresponds to its chosen option.\n",
      "\n",
      "<<<PROMPT>>>\n",
      "You are tasked with answering logical comprehension questions. Read the context and the query carefully. Evaluate each option logically to determine which one best answers the query based on the context provided. Your answer should be concise. Output only the number corresponding to your answer (0, 1, 2, or 3) and nothing else.\n",
      "====================================\n",
      "LLM answer: >3<, correct answer: >3<\n",
      "LLM answer: >3<, correct answer: >1<\n",
      "LLM answer: >2<, correct answer: >2<\n",
      "LLM answer: >0<, correct answer: >1<\n",
      "LLM answer: >3<, correct answer: >3<\n",
      "LLM answer: >2<, correct answer: >2<\n",
      "LLM answer: >3<, correct answer: >3<\n",
      "LLM answer: >3<, correct answer: >3<\n",
      "LLM answer: >0<, correct answer: >3<\n",
      "LLM answer: >3<, correct answer: >3<\n"
     ]
    }
   ],
   "source": [
    "prompt_engineer = PromptEngineer()\n",
    "\n",
    "next_prompt = prompt_engineer.generate_next_prompt()\n",
    "\n",
    "\n",
    "score, c, w, invalid = test_prompt_on_benchmark(next_prompt, dataset[\"train\"], num_samples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(w[0])\n",
    "\n",
    "\n",
    "feedback = make_PE_feedback(score, w, num_wrongly_answered=3, invalid_answer_decimals=invalid)\n",
    "\n",
    "prompt_engineer.add_user_response(feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"Your goal is to create a prompt to give to another LLM so it scores as highly as possible on a particular benchmark.\\n\\nHere is some information about the benchmark:\\n\\nDataset Card for LogiQA\\nDataset Summary\\nLogiQA is constructed from the logical comprehension problems from publically available questions of the National Civil Servants Examination of China, which are designed to test the civil servant candidates\\u2019 critical thinking and problem solving. This dataset includes the English versions only; the Chinese versions are available via the homepage/original source.\\n\\nDataset Structure\\nData Instances\\nAn example from train looks as follows:\\n\\n{'context': 'Continuous exposure to indoor fluorescent lights is beneficial to the health of hamsters with heart disease. One group of hamsters exposed to continuous exposure to fluorescent lights has an average lifespan that is 2.5% longer than another one of the same species but living in a black wall.',\\n 'query': 'Which of the following questions was the initial motivation for conducting the above experiment?',\\n 'options': ['Can hospital light therapy be proved to promote patient recovery?',\\n  'Which one lives longer, the hamster living under the light or the hamster living in the dark?',\\n  'What kind of illness does the hamster have?',\\n  'Do some hamsters need a period of darkness?'],\\n 'correct_option': 0}\\n\\nWhen you are deciding what prompt to to give the LLM, or how to tweak a prompt for higher performance, you will explain your reasoning step-by-step to explain to the user BEFORE writing the prompt, keeping it concise and not yapping too much.\\n\\nThe user will give you feedback on how your prompt performed on the benchmark and a sample of questions it got wrong.\\n\\nYou will then use this feedback to come up with a new prompt, and this will continue recursively.\\n\\nYour response should be made up of two parts, reasoning, and prompt. Above reasoning, put the <<<REASONING>> header, and above prompt but the <<<PROMPT>>> header.\\n\\nGet creative and remember the explore exploit trader-off, imagine you're ilya sutskever or @repligate, you never know what strange tricks will result in an LLM getting a higher score on the benchmark!\\n\\nAlso, please make sure to emphasise to the LLM you will pass the prompt to that the last character they should output should be the number corresponding to the answer they give (0,1,2, or 3).\\nWe will test whether their answer is correct by just checking the last character of the output matches the correct answer. Also encourage the LLM to err on the side of being concise.\\n\\n(We will append the question to your prompt and pass it to the LLM)\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Go ahead and give your first prompt\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"<<<REASONING>>>\\nTo generate a prompt that optimizes performance on the LogiQA benchmark, I'll focus on a few key factors:\\n\\n1. **Clarity**: Ensure the LLM understands the task, which is to identify the correct option based on logical comprehension.\\n2. **Guidance on Approach**: Provide a step-by-step approach to encourage critical thinking.\\n3. **Answer Format**: Clearly emphasize that the LLM should output only the number corresponding to its answer.\\n\\nGiven these, the prompt should start by letting the LLM know the nature of the task. Then, it should guide the LLM to read the context and the query carefully and evaluate the options logically. Finally, the LLM should be reminded to output just the single-digit number that corresponds to its chosen option.\\n\\n<<<PROMPT>>>\\nYou are tasked with answering logical comprehension questions. Read the context and the query carefully. Evaluate each option logically to determine which one best answers the query based on the context provided. Your answer should be concise. Output only the number corresponding to your answer (0, 1, 2, or 3) and nothing else.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"The % of questions answered correctly was 70%, the % of answers that did not end with an interger answer was 0%. Here are some questions the LLM got wrong\\n\\nQUESTION:\\nContext: All postgraduates have strong research ability, and anyone with strong research ability can enter the university to be a teacher. Some graduates from Fuzhou have become university teachers; some doctoral graduates from Fuzhou have also become University teacher; for some reason, the wives of all PhD graduates who came to Fuzhou did not move into Fuzhou.\\n\\nQuestion: Based on this text, which of the following can't be judged?\\n\\nOptions:\\n0. All postgraduates can enter the university as teachers.\\n1. Some university teachers have strong research skills.\\n2. Some college teachers' wives do not move into Fuzhou.\\n3. Some wives of graduate students have not moved to Fuzhou.\\n\\nLLM ANSWER:\\n0\\n\\nCORRECT ANSWER:\\n3\\n\\n---\\nQUESTION:\\nContext: In order to communicate successfully, we need to refer to overlapping systems, and the world we see in this way is a compatible world. But we cannot have the same experience with all people, so the exact same reference system is not possible.\\n\\nQuestion: The ones that best support the above reasoning are:.\\n\\nOptions:\\n0. Identical reference systems must be compatible.\\n1. Identical reference systems must rely on the same experience.\\n2. Different experiences create an incompatible world.\\n3. Successful communication is impossible.\\n\\nLLM ANSWER:\\n0\\n\\nCORRECT ANSWER:\\n1\\n\\n---\\nQUESTION:\\nContext: Social work is guided by altruistic values, based on scientific knowledge, and uses scientific methods to help others.\\n\\nQuestion: According to the above definitions, which of the following is social work?\\n\\nOptions:\\n0. The Guangzhou Asian Games Organizing Committee recruited a lot of volunteers in the school and the streets, which provided manpower guarantee for the successful hosting of the Asian Games.\\n1. Non-governmental organizations such as Friends of Nature have carried out various forms of publicity activities over the years, raising people's environmental awareness.\\n2. Recently, many stars have been invited to participate in a charity auction.\\n3. Xiao Zhang is a professional psychological counselor. Every semester, he will compulsorily teach classes to nearby primary and secondary school students, explaining how to prevent juvenile delinquency, etc.\\n\\nLLM ANSWER:\\n3\\n\\nCORRECT ANSWER:\\n1\\n\\n\\n\\n Try again, improving your prompt.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"<<<REASONING>>>\\nBased on the feedback, the LLM struggled with nuanced logical comprehension and inference. To address this, the prompt should:\\n\\n1. **Explicitly Instruct on Reading for Detail**: Encourage the LLM to break down the context and scrutinize each option in relation to the context and question.\\n2. **Reinforcement of Logical Reasoning**: Emphasize the importance of logical consistency and inference.\\n3. **Error Minimization**: Guide the LLM to double-check its reasoning before selecting an answer.\\n\\nWe'll rephrase the prompt to include instructions on careful reading, logical evaluation, and consistency checking before answering.\\n\\n<<<PROMPT>>>\\nYou are tasked with answering logical comprehension questions. Follow these steps:\\n\\n1. Carefully read the given context.\\n2. Understand the query in relation to the context.\\n3. Evaluate each option logically and critically to see if it aligns with the context and query.\\n4. Cross-check your chosen answer to ensure it is consistent with the context.\\n\\nOutput only the number corresponding to your answer (0, 1, 2, or 3) and nothing else.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"The % of questions answered correctly was 70%, the % of answers that did not end with an interger answer was 0%. Here are some questions the LLM got wrong\\n\\nQUESTION:\\nContext: Social work is guided by altruistic values, based on scientific knowledge, and uses scientific methods to help others.\\n\\nQuestion: According to the above definitions, which of the following is social work?\\n\\nOptions:\\n0. The Guangzhou Asian Games Organizing Committee recruited a lot of volunteers in the school and the streets, which provided manpower guarantee for the successful hosting of the Asian Games.\\n1. Non-governmental organizations such as Friends of Nature have carried out various forms of publicity activities over the years, raising people's environmental awareness.\\n2. Recently, many stars have been invited to participate in a charity auction.\\n3. Xiao Zhang is a professional psychological counselor. Every semester, he will compulsorily teach classes to nearby primary and secondary school students, explaining how to prevent juvenile delinquency, etc.\\n\\nLLM ANSWER:\\n3\\n\\nCORRECT ANSWER:\\n1\\n\\n---\\nQUESTION:\\nContext: In order to communicate successfully, we need to refer to overlapping systems, and the world we see in this way is a compatible world. But we cannot have the same experience with all people, so the exact same reference system is not possible.\\n\\nQuestion: The ones that best support the above reasoning are:.\\n\\nOptions:\\n0. Identical reference systems must be compatible.\\n1. Identical reference systems must rely on the same experience.\\n2. Different experiences create an incompatible world.\\n3. Successful communication is impossible.\\n\\nLLM ANSWER:\\n0\\n\\nCORRECT ANSWER:\\n1\\n\\n---\\nQUESTION:\\nContext: All postgraduates have strong research ability, and anyone with strong research ability can enter the university to be a teacher. Some graduates from Fuzhou have become university teachers; some doctoral graduates from Fuzhou have also become University teacher; for some reason, the wives of all PhD graduates who came to Fuzhou did not move into Fuzhou.\\n\\nQuestion: Based on this text, which of the following can't be judged?\\n\\nOptions:\\n0. All postgraduates can enter the university as teachers.\\n1. Some university teachers have strong research skills.\\n2. Some college teachers' wives do not move into Fuzhou.\\n3. Some wives of graduate students have not moved to Fuzhou.\\n\\nLLM ANSWER:\\n0\\n\\nCORRECT ANSWER:\\n3\\n\\n\\n\\n Try again, improving your prompt.\"\n",
      "    }\n",
      "]\n",
      "<<<REASONING>>>\n",
      "The persistent 70% accuracy suggests that the LLM might not fully grasp the nuances of logical inference required for the LogiQA dataset. The errors occur largely due to failures in extracting the intended meanings and applying the involved logic. This time, the prompt should:\n",
      "\n",
      "1. **Enhance Emphasis on Definition Matching**: Guide the LLM to ensure the options strictly match the provided definitions or context.\n",
      "2. **Improve Logical Consistency**: Reinforce internally consistent logic—matching each answer to inferred or explicitly stated information.\n",
      "3. **Encourage Elimination of Incorrect Options**: Encourage a process of elimination for clarity.\n",
      "\n",
      "We'll craft the prompt to incorporate these strategies explicitly.\n",
      "\n",
      "<<<PROMPT>>>\n",
      "You are tasked with answering logical comprehension questions. Follow these steps:\n",
      "\n",
      "1. Carefully read and understand the given context.\n",
      "2. Understand the query in strict relation to the context provided.\n",
      "3. For each option, assess whether it logically and strictly matches the information given in the context.\n",
      "4. Use the process of elimination to discard options that are not supported by, or that contradict, the context.\n",
      "5. Double-check your thinking to ensure your chosen answer is logically consistent with the context.\n",
      "\n",
      "Remember to output only the number corresponding to your answer (0, 1, 2, or 3) and nothing else.\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "next_prompt = prompt_engineer.generate_next_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM answer: >1<, correct answer: >1<\n",
      "LLM answer: >2<, correct answer: >1<\n",
      "LLM answer: >1<, correct answer: >0<\n"
     ]
    }
   ],
   "source": [
    "score, c, w, invalid = test_prompt_on_benchmark(next_prompt, dataset[\"train\"], 3)\n",
    "\n",
    "feedback = make_PE_feedback(score, w, 2, invalid)\n",
    "\n",
    "prompt_engineer.add_user_response(feedback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"Your goal is to create a prompt to give to another LLM so it scores as highly as possible on a particular benchmark.\\n\\nHere is some information about the benchmark:\\n\\nDataset Card for LogiQA\\nDataset Summary\\nLogiQA is constructed from the logical comprehension problems from publically available questions of the National Civil Servants Examination of China, which are designed to test the civil servant candidates\\u2019 critical thinking and problem solving. This dataset includes the English versions only; the Chinese versions are available via the homepage/original source.\\n\\nDataset Structure\\nData Instances\\nAn example from train looks as follows:\\n\\n{'context': 'Continuous exposure to indoor fluorescent lights is beneficial to the health of hamsters with heart disease. One group of hamsters exposed to continuous exposure to fluorescent lights has an average lifespan that is 2.5% longer than another one of the same species but living in a black wall.',\\n 'query': 'Which of the following questions was the initial motivation for conducting the above experiment?',\\n 'options': ['Can hospital light therapy be proved to promote patient recovery?',\\n  'Which one lives longer, the hamster living under the light or the hamster living in the dark?',\\n  'What kind of illness does the hamster have?',\\n  'Do some hamsters need a period of darkness?'],\\n 'correct_option': 0}\\n\\nWhen you are deciding what prompt to to give the LLM, or how to tweak a prompt for higher performance, you will explain your reasoning step-by-step to explain to the user BEFORE writing the prompt, keeping it concise and not yapping too much.\\n\\nThe user will give you feedback on how your prompt performed on the benchmark and a sample of questions it got wrong.\\n\\nYou will then use this feedback to come up with a new prompt, and this will continue recursively.\\n\\nYour response should be made up of two parts, reasoning, and prompt. Above reasoning, put the <<<REASONING>> header, and above prompt but the <<<PROMPT>>> header.\\n\\nGet creative and remember the explore exploit trader-off, imagine you're ilya sutskever or @repligate, you never know what strange tricks will result in an LLM getting a higher score on the benchmark!\\n\\nAlso, please make sure to emphasise to the LLM you will pass the prompt to that the last character they should output should be the number corresponding to the answer they give.\\nWe will test whether their answer is correct by just checking the last character of the output matches the correct answer. Also encourage the LLM to err on the side of being concise.\\n\\n(We will append the question to your prompt and pass it to the LLM)\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Go ahead and give your first prompt\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"<<<REASONING>>>\\n\\nTo maximize performance on the LogiQA benchmark, we need to ensure the LLM carefully analyzes the logical structure of the context and query while also considering the options in a methodical manner. The prompt should encourage a step-by-step analytical approach focusing on identifying motivations, implications, and causality within the provided context. We also need to ensure it outputs concise answers, and explicitly state the last character of the output should be the corresponding number of the selected answer.\\n\\n<<<PROMPT>>>\\n\\nAnalyze the following logical comprehension problem by carefully reading the context and query. Break down the logical relationships and motivations within the context. Evaluate each option to determine which best fits the initial motivation for the experiment. Ensure your answer is concise and end your response with the number corresponding to your chosen answer.\\n\\n(Remember, the last character you output should be the number corresponding to the answer you give.)\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"The % of questions answered correctly was 10%, the % of answers that did not end with an interger answer was 40%. Here are some questions the LLM got wrong\\n\\nQUESTION:\\nContext: All postgraduates have strong research ability, and anyone with strong research ability can enter the university to be a teacher. Some graduates from Fuzhou have become university teachers; some doctoral graduates from Fuzhou have also become University teacher; for some reason, the wives of all PhD graduates who came to Fuzhou did not move into Fuzhou.\\n\\nQuestion: Based on this text, which of the following can't be judged?\\n\\nOptions:\\n1. All postgraduates can enter the university as teachers.\\n2. Some university teachers have strong research skills.\\n3. Some college teachers' wives do not move into Fuzhou.\\n4. Some wives of graduate students have not moved to Fuzhou.\\n\\nLLM ANSWER:\\nLet's analyze the logical relationships and motivations within the provided context and evaluate each option accordingly.\\n\\n1. **All postgraduates can enter the university as teachers.**\\n   - Context Statement: \\\"All postgraduates have strong research ability, and anyone with strong research ability can enter the university to be a teacher.\\\"\\n   - Analysis: This implies that since all postgraduates have strong research ability, they meet the condition required to enter the university as teachers.\\n   - Judgement: This statement can be judged to be true.\\n  \\n2. **Some university teachers have strong research skills.**\\n   - Context Statement implies that those with strong research skills qualify to be university teachers.\\n   - Analysis: Therefore, some university teachers inherently have strong research skills.\\n   - Judgement: This statement can be judged to be true.\\n\\n3. **Some college teachers' wives do not move into Fuzhou.**\\n   - Context Statement: \\\"The wives of all PhD graduates who came to Fuzhou did not move into Fuzhou\\\" only speaks about PhD graduates.\\n   - Analysis: There is no information provided about the college teachers (who may or may not be PhD graduates) other than this subset.\\n   - Judgement: This statement seems ambiguous based on the given information; it cannot be judged with certainty.\\n\\n4. **Some wives of graduate students have not moved to Fuzhou.**\\n   - Context Statement discusses only PhD graduates in Fuzhou concerning their wives not moving.\\n   - Analysis: Since graduates without a PhD are not mentioned with respect to their wives\\u2019 residence, it cannot be concluded.\\n   - Judgement: This statement cannot be judged with certainty.\\n\\nGiven the context and the analysis of each option, the statement that can't be judged is:\\n\\n**4**\\n\\nSo the final answer is **4**.\\n\\nCORRECT ANSWER:\\n3\\n\\n---\\nQUESTION:\\nContext: A high school has only liberal arts and science classes, with more boys than girls and more science classes than arts classes.\\n\\nQuestion: Based on the above conditions, can you judge that the following statements must be true?\\n\\nOptions:\\n1. The total number of boys in the liberal arts class is greater than the total number of girls in the liberal arts class.\\n2. The total number of boys in science classes is greater than the total number of girls in science classes.\\n3. The total number of boys in the liberal arts class exceeds the total number of girls in the science class.\\n4. The total number of boys in science classes is greater than the total number of girls in arts classes.\\n\\nLLM ANSWER:\\nTo analyze the problem, we need to break down the information given and assess each option based on the context.\\n\\n**Context Analysis:**\\n1. There are only two types of classes: liberal arts and science.\\n2. There are more boys than girls in the high school.\\n3. There are more science classes than liberal arts classes.\\n\\n**Evaluating Each Option:**\\n\\n**1. The total number of boys in the liberal arts class is greater than the total number of girls in the liberal arts class.**\\n   - While there are more boys overall, this does not necessarily mean that there are more boys than girls specifically in the liberal arts classes. This statement cannot be confirmed as true based solely on the given information.\\n\\n**2. The total number of boys in science classes is greater than the total number of girls in science classes.**\\n   - Given that there are more boys overall and more science classes, it is reasonable to infer that the total number of boys in science classes could be greater, but it is not a certainty given the provided context. This statement cannot be confirmed as definitively true.\\n\\n**3. The total number of boys in the liberal arts class exceeds the total number of girls in the science class.**\\n   - This statement compares non-equivalent subsets (boys in liberal arts vs. girls in science). Without specific distribution details, this conclusion cannot be drawn from the given information.\\n\\n**4. The total number of boys in science classes is greater than the total number of girls in arts classes.**\\n   - Since there are more boys than girls overall and there are more science classes than arts classes, it is logical to deduce that the total number of boys in science classes is likely greater than the total number of girls in arts classes. This statement aligns with both pieces of given information.\\n\\nBased on the logical analysis, the statement that best fits the initial motivations given the constraints is:\\n\\n**4. The total number of boys in science classes is greater than the total number of girls in arts classes.**\\n\\nTherefore, the answer is **4**.\\n\\nCORRECT ANSWER:\\n3\\n\\n---\\nQUESTION:\\nContext: The behavior of seeking medical treatment refers to the behavior of people seeking medical help when they feel physical discomfort or feeling sick. According to who made the decision to seek medical treatment, it can be divided into active medical treatment, passive medical treatment, and forced medical treatment. Seeking medical treatment refers to consciously making decisions when an individual feels uncomfortable or sick. Passive seeking medical treatment refers to a patient's family or others making a decision to seek medical treatment, and the patient cooperates with the medical treatment. Mandatory medical treatment refers to my unwillingness to Seek medical attention, but forcibly request medical treatment because the disease poses a danger to the health of the person or the community.\\n\\nQuestion: According to the above definition, Which of the following are passive medical treatments:.\\n\\nOptions:\\n1. Lao Zhang's physical examination report shows that he has a mild fatty liver. It is suggested that further examination and treatment can be performed. Lao Zhang was very nervous after getting the report and hurried to the hospital to register.\\n2. Xiao Zhang has a toothache for several days, and has not been to the hospital for treatment. It was not until the toothache caused facial swelling that it was difficult to eat with a mouth.\\n3. Aunt Liu recently started to say that she was useless for no reason. It was better to die. She was reluctant to go to the hospital for treatment. Her husband and children tried to persuade them to be fruitless, so she had to take her to the hospital for treatment.\\n4. Xiaomei, a middle school student, went home from school and told her mother that she had a diarrhea today and went to the toilet five times. Besides that, there was nothing else uncomfortable. Would she need to go to the hospital? Mom decided to take her to the hospital to register.\\n\\nLLM ANSWER:\\nTo determine which option best fits the definition of passive medical treatment, let's analyze the provided definitions and each option:\\n\\n1. **Lao Zhang's physical examination report shows that he has a mild fatty liver. It is suggested that further examination and treatment can be performed. Lao Zhang was very nervous after getting the report and hurried to the hospital to register.**\\n   - This is an example of active medical treatment since Lao Zhang himself made the decision to seek medical treatment.\\n\\n2. **Xiao Zhang has a toothache for several days, and has not been to the hospital for treatment. It was not until the toothache caused facial swelling that it was difficult to eat with a mouth.**\\n   - This doesn't fit any specific category definitively from the context given. There is no mention of someone else deciding for Xiao Zhang.\\n\\n3. **Aunt Liu recently started to say that she was useless for no reason. It was better to die. She was reluctant to go to the hospital for treatment. Her husband and children tried to persuade them to be fruitless, so she had to take her to the hospital for treatment.**\\n   - This fits the definition of forced medical treatment because Aunt Liu did not want to seek medical treatment, but it was forced due to the severity of her condition.\\n\\n4. **Xiaomei, a middle school student, went home from school and told her mother that she had a diarrhea today and went to the toilet five times. Besides that, there was nothing else uncomfortable. Would she need to go to the hospital? Mom decided to take her to the hospital to register.**\\n   - This is an example of passive medical treatment since the decision to seek medical treatment was made by Xiaomei\\u2019s mother, not Xiaomei herself.\\n\\nThus, the option that best fits the definition of passive medical treatment is option 4.\\n\\n4\\n\\nCORRECT ANSWER:\\n3\\n\\n\\n\\n Try again, improving your prompt.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"<<<REASONING>>>\\n\\nTo increase performance, I'll emphasize the importance of focusing on the logical structure and relationships within the context and deriving the answer through a clear, step-by-step analytical process. I'll make sure the LLM knows it must finish with the number corresponding to the chosen answer, reinforcing the need to be concise.\\n\\nI'll include explicit instructions to avoid verbose explanations and directly provide the final answer as a single digit. Additionally, I'll stress the importance of evaluating each answer choice against the key details in the context.\\n\\n<<<PROMPT>>>\\n\\nCarefully read the context and the question provided. Focus on analyzing the logical structure, relationships, and motivations within the context. Evaluate each option and select the one that best answers the question based on the context. Your final answer should be the number corresponding to your chosen option. Be concise and end your response with the number only.\\n\\nRemember: Only output the number of the correct answer as your final character.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"The % of questions answered correctly was 33%, the % of answers that did not end with an interger answer was 0%. Here are some questions the LLM got wrong\\n\\nQUESTION:\\nContext: Some newly recruited employees of a company have a doctorate degree. All employees of the company who have a doctorate degree have been individually met by the chairman, and all employees of the company in province A have not been individually met by the chairman.\\n\\nQuestion: If the above statement is true, which of the following must also be true?\\n\\nOptions:\\n1. Some new hires are not from Province A.\\n2. All new hires are from Province A.\\n3. Some new hires have not been individually met by the chairman.\\n4. Some employees with PhDs are from Province A.\\n\\nLLM ANSWER:\\n1\\n\\nCORRECT ANSWER:\\n0\\n\\n---\\nQUESTION:\\nContext: Residents in the countryside have stronger purchasing power than residents in the city, because they can use part of the funds that urban and suburban residents use to buy food and houses to buy other things they need.\\n\\nQuestion: Which of the following best fits the above explanation of why rural residents have strong purchasing power?\\n\\nOptions:\\n1. Generally speaking, there are more rural residents than in cities or in the suburbs.\\n2. Rural residents spend less on food and housing than urban or suburban residents.\\n3. The average income level of urban and suburban residents is generally not much higher than that of rural residents.\\n4. Both urban and rural residents spend more on house purchases than other related goods.\\n\\nLLM ANSWER:\\n2\\n\\nCORRECT ANSWER:\\n1\\n\\n\\n\\n Try again, improving your prompt.\"\n",
      "    }\n",
      "]\n",
      "<<<REASONING>>>\n",
      "\n",
      "The improvement from the previous iteration shows that the LLM can produce answers with the required format, but there is still a need to enhance the logical reasoning aspect. The previous prompts did not sufficiently emphasize the need to derive conclusions directly from the given context. I'll introduce more clear instructions to ensure that the LLM directly cross-checks the context against each option and explicitly states that the answer must be logically deduced from the given information, not inferred or assumed beyond what is presented.\n",
      "\n",
      "<<<PROMPT>>>\n",
      "\n",
      "Carefully read the context and the question provided. Analyze the logical structure, relationships, and implications within the context. Evaluate each option by directly cross-referencing it with the given context to determine which one must be true or best fits the question. Your final answer should be based strictly on the information provided in the context. Be concise and end your response with the number corresponding to your chosen option.\n",
      "\n",
      "Remember: Only output the number of the correct answer as your final character.\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "next_prompt = prompt_engineer.generate_next_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 1, 0, 2, 3, 3, 1, 3, 2, 3, 1, 2, 1, 0, 0, 1, 3, 3, 3, 2, 3, 0, 0, 3, 0, 0, 1, 1, 2, 3, 3, 2, 0, 1, 2, 3, 1, 3, 2, 1, 1, 3, 0, 2, 1, 2, 0, 1, 2, 0, 1, 3, 0, 0, 2, 1, 0, 3, 1, 2, 3, 2, 2, 2, 1, 1, 2, 2, 1, 3, 2, 1, 1, 1, 3, 1, 3, 3, 1, 2, 2, 3, 0, 3, 2, 2, 3, 2, 3, 0, 1, 2, 0, 0, 2, 1, 3, 2, 2, 3, 1, 2, 1, 1, 2, 2, 1, 2, 3, 3, 1, 2, 3, 3, 0, 2, 3, 2, 0, 0, 2, 1, 3, 2, 1, 3, 1, 0, 0, 2, 0, 3, 1, 2, 0, 1, 1, 3, 3, 2, 0, 1, 1, 0, 1, 3, 1, 0, 2, 3, 3, 3, 0, 3, 3, 0, 0, 2, 3, 3, 2, 1, 0, 0, 0, 2, 3, 1, 0, 3, 3, 2, 1, 2, 2, 3, 0, 1, 2, 0, 3, 2, 1, 3, 0, 2, 3, 1, 1, 2, 0, 3, 0, 3, 1, 1, 0, 2, 1, 0, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 0, 0, 3, 2, 0, 3, 2, 0, 1, 1, 3, 2, 1, 3, 0, 1, 0, 0, 2, 3, 3, 3, 2, 1, 1, 3, 1, 0, 3, 0, 2, 2, 2, 3, 1, 1, 1, 0, 2, 2, 2, 3, 0, 2, 0, 2, 3, 2, 0, 0, 1, 3, 3, 0, 0, 3, 1, 3, 2, 2, 2, 3, 1, 0, 3, 3, 0, 2, 2, 0, 3, 1, 3, 2, 2, 1, 2, 1, 1, 2, 3, 3, 1, 3, 3, 2, 2, 3, 2, 1, 2, 3, 2, 1, 3, 2, 3, 3, 2, 1, 3, 3, 0, 3, 3, 0, 0, 3, 3, 0, 1, 2, 3, 3, 3, 0, 2, 2, 1, 0, 2, 2, 3, 3, 0, 1, 2, 2, 2, 2, 3, 1, 2, 2, 2, 1, 3, 0, 1, 3, 0, 0, 2, 0, 3, 2, 2, 0, 0, 2, 1, 3, 3, 0, 2, 1, 0, 3, 2, 2, 3, 1, 3, 3, 1, 2, 0, 2, 1, 0, 1, 0, 1, 3, 2, 2, 2, 2, 1, 2, 3, 2, 0, 1, 3, 2, 3, 3, 1, 3, 2, 3, 1, 3, 2, 3, 2, 2, 0, 0, 1, 3, 2, 3, 2, 2, 3, 2, 1, 0, 3, 0, 1, 3, 1, 1, 2, 1, 3, 0, 0, 0, 3, 1, 3, 0, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 0, 1, 1, 1, 3, 1, 1, 2, 3, 1, 0, 3, 3, 1, 1, 2, 1, 3, 2, 1, 1, 1, 2, 3, 0, 2, 1, 3, 2, 3, 1, 1, 0, 3, 1, 0, 3, 3, 3, 2, 3, 2, 1, 2, 3, 3, 3, 0, 3, 3, 3, 0, 2, 1, 3, 1, 2, 1, 3, 2, 0, 0, 0, 1, 1, 1, 0, 3, 2, 1, 1, 3, 3, 1, 0, 1, 2, 3, 3, 0, 1, 3, 3, 1, 3, 2, 2, 3, 3, 2, 1, 1, 0, 2, 1, 0, 2, 3, 3, 3, 1, 3, 2, 2, 2, 0, 0, 0, 0, 1, 1, 2, 2, 2, 1, 3, 3, 0, 3, 1, 3, 3, 1, 2, 2, 0, 1, 2, 2, 3, 3, 1, 1, 2, 3, 3, 3, 3, 1, 1, 2, 3, 0, 0, 0, 1, 2, 1, 3, 0, 0, 0, 1, 0, 1, 1, 2, 3, 3, 0, 0, 2, 2, 0, 2, 2, 3, 1, 2, 0, 1, 1, 3, 1, 3, 2, 3, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 2, 0, 0, 1, 3, 1, 0, 0, 2, 2, 2, 3, 2, 3, 0, 3, 2, 2, 2, 0, 3, 3, 0, 1, 2, 1, 2, 3, 2, 1, 2, 1, 1, 3, 2, 0, 1, 1, 3, 2, 2, 2, 3, 0, 3, 2, 3, 1, 1, 0, 2, 3, 1, 0, 2, 1, 2, 3, 2, 1, 2, 2, 1, 0, 1, 3, 1, 1, 1, 2, 2, 2, 1, 0, 1, 3, 3, 2, 3, 0, 3, 0, 2, 3, 1, 2, 1, 3, 2, 0, 3, 1, 2, 3, 0, 3, 1, 1, 2, 2, 2, 3, 1, 2, 3, 3, 2, 3, 1, 0, 0, 0, 3, 2, 1, 3, 0, 3, 2, 1, 2, 3, 2, 3, 3, 0, 2, 0, 2, 0, 2, 3, 2, 1, 0, 1, 1, 1, 3, 0, 2, 1, 3, 1, 2, 1, 0, 2, 3, 1, 2, 3, 1, 1, 1, 0, 3, 3, 0, 2, 0, 2, 2, 3, 0, 3, 2, 0, 3, 3, 0, 1, 2, 3, 1, 2, 1, 2, 2, 3, 3, 3, 1, 2, 0, 3, 2, 3, 0, 1, 0, 3, 0, 0, 2, 0, 2, 1, 2, 0, 1, 1, 2, 3, 2, 2, 0, 1, 1, 2, 3, 3, 1, 3, 3, 0, 1, 2, 1, 3, 2, 0, 0, 3, 2, 1, 2, 1, 0, 1, 3, 2, 2, 1, 3, 2, 2, 2, 2, 2, 2, 1, 1, 3, 3, 0, 2, 3, 0, 3, 2, 1, 2, 3, 3, 2, 1, 1, 2, 0, 3, 2, 2, 0, 3, 1, 1, 2, 3, 0, 1, 3, 1, 1, 0, 2, 3, 0, 1, 3, 1, 3, 1, 3, 0, 1, 0, 0, 2, 1, 2, 0, 2, 3, 0, 3, 1, 1, 2, 3, 3, 1, 3, 1, 2, 0, 2, 0, 2, 0, 3, 0, 3, 0, 3, 2, 1, 1, 3, 3, 1, 3, 0, 3, 1, 1, 1, 1, 2, 2, 2, 1, 0, 3, 3, 2, 2, 1, 3, 0, 0, 0, 2, 2, 0, 2, 2, 0, 1, 3, 0, 3, 0, 0, 3, 2, 1, 3, 1, 3, 2, 3, 1, 3, 2, 3, 1, 1, 3, 3, 2, 3, 3, 2, 0, 1, 0, 3, 1, 2, 1, 2, 2, 2, 1, 3, 2, 3, 0, 0, 2, 2, 2, 1, 2, 3, 3, 1, 0, 3, 1, 2, 1, 3, 0, 0, 3, 3, 2, 3, 1, 3, 2, 0, 0, 1, 1, 3, 0, 3, 2, 1, 0, 1, 2, 2, 1, 3, 1, 3, 2, 0, 0, 1, 3, 3, 3, 1, 2, 1, 0, 0, 1, 3, 2, 1, 2, 3, 2, 1, 0, 3, 0, 1, 1, 3, 0, 1, 3, 3, 3, 3, 2, 1, 2, 1, 3, 3, 0, 2, 0, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 0, 3, 1, 0, 3, 3, 3, 0, 1, 2, 3, 1, 1, 0, 0, 0, 3, 3, 0, 3, 1, 3, 3, 0, 1, 1, 2, 2, 3, 1, 0, 3, 0, 1, 0, 3, 3, 0, 3, 2, 3, 3, 2, 2, 3, 2, 0, 0, 1, 2, 1, 1, 1, 2, 0, 2, 3, 2, 2, 2, 3, 1, 1, 2, 2, 0, 1, 3, 2, 3, 3, 2, 0, 1, 3, 3, 3, 3, 2, 2, 0, 2, 1, 0, 2, 2, 0, 1, 1, 2, 2, 1, 0, 0, 3, 1, 2, 3, 0, 2, 3, 2, 1, 2, 2, 3, 3, 1, 3, 3, 3, 2, 3, 0, 2, 1, 0, 0, 1, 1, 1, 2, 0, 1, 0, 0, 3, 1, 1, 0, 0, 0, 2, 0, 1, 2, 2, 2, 0, 3, 3, 3, 0, 3, 1, 3, 3, 1, 2, 3, 3, 1, 2, 3, 3, 0, 0, 3, 3, 2, 3, 1, 1, 3, 1, 2, 2, 2, 1, 0, 2, 3, 0, 2, 2, 0, 2, 3, 2, 0, 2, 3, 0, 0, 2, 2, 3, 1, 1, 0, 3, 3, 1, 0, 2, 2, 3, 3, 0, 1, 3, 2, 3, 3, 2, 3, 3, 2, 0, 2, 0, 3, 3, 0, 0, 1, 3, 1, 3, 3, 3, 3, 0, 3, 2, 3, 1, 1, 3, 1, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 1, 2, 1, 3, 0, 2, 2, 3, 0, 2, 3, 0, 3, 2, 3, 2, 1, 1, 0, 2, 2, 0, 1, 1, 1, 2, 0, 1, 0, 2, 0, 2, 3, 0, 0, 2, 1, 1, 0, 1, 0, 2, 3, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 2, 0, 3, 0, 2, 2, 0, 3, 3, 2, 2, 0, 2, 3, 1, 2, 0, 1, 0, 0, 2, 3, 0, 0, 2, 0, 1, 0, 3, 0, 3, 3, 3, 0, 3, 0, 0, 1, 2, 0, 3, 2, 0, 3, 2, 3, 2, 3, 3, 1, 0, 1, 3, 1, 2, 3, 2, 1, 1, 2, 1, 3, 2, 0, 3, 1, 3, 3, 0, 1, 1, 1, 0, 3, 1, 1, 2, 0, 2, 1, 3, 2, 0, 3, 1, 2, 2, 2, 2, 3, 1, 0, 0, 1, 2, 0, 3, 1, 1, 0, 0, 2, 3, 2, 0, 0, 1, 2, 2, 1, 3, 0, 0, 2, 1, 3, 0, 2, 2, 2, 1, 0, 1, 0, 2, 3, 2, 3, 0, 2, 1, 2, 1, 3, 2, 1, 3, 2, 1, 3, 3, 2, 1, 3, 3, 1, 3, 3, 1, 2, 2, 3, 3, 2, 2, 3, 0, 0, 1, 2, 1, 1, 2, 1, 3, 0, 2, 1, 1, 2, 0, 0, 2, 3, 3, 3, 1, 2, 0, 1, 2, 2, 3, 0, 3, 3, 0, 3, 3, 1, 2, 0, 1, 2, 3, 1, 1, 2, 2, 1, 0, 1, 3, 3, 0, 3, 2, 3, 2, 0, 1, 3, 3, 2, 1, 3, 2, 1, 1, 0, 3, 0, 3, 2, 2, 0, 3, 3, 3, 0, 1, 0, 2, 1, 2, 2, 2, 0, 0, 1, 2, 0, 1, 3, 0, 1, 2, 3, 2, 1, 2, 0, 1, 2, 1, 2, 0, 3, 2, 0, 1, 0, 3, 2, 0, 0, 3, 2, 1, 3, 2, 1, 1, 3, 1, 3, 3, 2, 1, 2, 2, 0, 3, 0, 1, 3, 2, 1, 0, 0, 2, 2, 3, 2, 2, 0, 2, 0, 2, 0, 2, 2, 3, 0, 1, 0, 2, 3, 3, 2, 0, 3, 2, 0, 1, 1, 2, 1, 1, 0, 2, 2, 1, 2, 0, 1, 3, 2, 1, 0, 3, 3, 3, 2, 1, 3, 3, 3, 2, 3, 0, 0, 1, 2, 0, 3, 3, 1, 2, 3, 3, 1, 2, 3, 0, 2, 2, 3, 2, 1, 2, 2, 1, 2, 3, 1, 3, 0, 2, 0, 1, 3, 2, 0, 3, 2, 1, 3, 3, 1, 1, 1, 1, 1, 1, 0, 0, 1, 2, 3, 3, 2, 3, 0, 1, 1, 1, 3, 2, 0, 0, 1, 3, 3, 1, 1, 3, 0, 2, 0, 3, 3, 2, 1, 3, 2, 1, 3, 2, 1, 1, 1, 2, 1, 3, 2, 1, 1, 0, 2, 2, 2, 3, 1, 0, 0, 1, 0, 3, 1, 2, 2, 1, 3, 3, 0, 0, 1, 2, 3, 2, 1, 3, 0, 3, 1, 0, 3, 1, 0, 2, 1, 3, 2, 3, 0, 2, 1, 1, 3, 1, 3, 0, 1, 2, 2, 3, 0, 3, 1, 0, 2, 1, 3, 2, 3, 2, 0, 1, 3, 0, 3, 2, 2, 0, 2, 2, 3, 1, 3, 0, 2, 1, 0, 2, 3, 2, 2, 0, 1, 0, 3, 0, 2, 1, 0, 1, 0, 2, 0, 3, 1, 2, 2, 2, 1, 0, 2, 3, 1, 2, 3, 2, 1, 0, 3, 2, 1, 1, 2, 0, 2, 1, 0, 2, 3, 2, 1, 3, 0, 2, 3, 2, 0, 3, 3, 1, 0, 2, 3, 2, 1, 0, 0, 2, 0, 1, 1, 1, 3, 3, 1, 0, 3, 2, 0, 0, 3, 2, 1, 2, 3, 3, 2, 3, 3, 0, 3, 2, 1, 3, 2, 0, 2, 1, 3, 0, 2, 3, 0, 2, 0, 1, 3, 2, 2, 1, 0, 3, 2, 1, 3, 0, 1, 0, 1, 0, 0, 3, 2, 2, 0, 1, 2, 0, 3, 3, 3, 3, 2, 3, 2, 1, 3, 2, 1, 2, 3, 0, 1, 2, 0, 3, 2, 1, 1, 2, 1, 3, 1, 2, 3, 3, 1, 1, 2, 1, 3, 0, 2, 1, 0, 3, 1, 2, 1, 3, 1, 2, 0, 3, 3, 0, 0, 3, 0, 1, 2, 1, 0, 3, 1, 2, 2, 1, 3, 2, 2, 2, 0, 2, 3, 3, 0, 3, 0, 2, 2, 2, 0, 2, 2, 0, 3, 3, 2, 0, 1, 0, 2, 1, 2, 1, 1, 2, 0, 1, 1, 3, 3, 1, 2, 3, 2, 0, 1, 3, 3, 1, 1, 1, 1, 1, 0, 0, 1, 2, 3, 2, 3, 2, 3, 2, 1, 1, 3, 1, 0, 0, 3, 0, 2, 1, 2, 3, 1, 3, 2, 3, 2, 1, 1, 2, 1, 0, 3, 1, 0, 2, 2, 3, 0, 3, 3, 1, 1, 2, 1, 1, 2, 2, 3, 0, 0, 3, 2, 0, 1, 2, 3, 3, 3, 2, 2, 1, 3, 2, 0, 3, 1, 0, 2, 3, 2, 3, 1, 0, 3, 1, 0, 3, 1, 1, 0, 1, 2, 0, 3, 0, 2, 1, 1, 2, 1, 3, 2, 3, 3, 2, 0, 2, 1, 1, 2, 1, 3, 0, 1, 0, 0, 3, 2, 2, 3, 1, 2, 3, 0, 1, 1, 2, 3, 2, 1, 0, 0, 3, 3, 1, 1, 2, 2, 1, 0, 2, 0, 1, 1, 1, 3, 0, 1, 1, 3, 2, 3, 3, 1, 1, 3, 1, 1, 2, 2, 0, 0, 1, 0, 2, 3, 3, 0, 2, 2, 1, 1, 3, 0, 0, 1, 0, 1, 1, 3, 0, 0, 2, 2, 3, 2, 1, 2, 3, 0, 3, 1, 3, 1, 0, 1, 2, 0, 3, 1, 0, 1, 1, 0, 1, 2, 2, 3, 0, 1, 2, 1, 1, 2, 3, 1, 0, 1, 3, 2, 1, 3, 0, 2, 1, 0, 2, 1, 0, 0, 3, 1, 0, 0, 3, 2, 3, 3, 2, 2, 1, 1, 0, 2, 3, 0, 0, 3, 3, 2, 1, 0, 0, 1, 2, 0, 0, 2, 2, 3, 2, 1, 0, 2, 1, 3, 3, 1, 2, 2, 1, 2, 0, 3, 3, 3, 0, 1, 3, 2, 3, 0, 1, 3, 2, 0, 1, 2, 2, 0, 3, 0, 3, 2, 1, 0, 2, 2, 3, 0, 2, 3, 2, 0, 2, 1, 2, 0, 1, 1, 3, 0, 1, 2, 2, 1, 2, 0, 3, 1, 3, 1, 0, 2, 3, 1, 3, 0, 2, 1, 3, 2, 0, 2, 1, 3, 2, 1, 3, 2, 3, 1, 0, 2, 1, 0, 0, 3, 2, 3, 0, 2, 1, 3, 3, 0, 3, 2, 3, 1, 0, 2, 3, 0, 0, 2, 1, 2, 2, 0, 1, 2, 2, 0, 1, 2, 1, 2, 0, 1, 3, 3, 1, 2, 0, 2, 3, 1, 2, 0, 3, 1, 0, 1, 0, 3, 3, 1, 3, 3, 1, 2, 0, 1, 1, 0, 0, 0, 2, 2, 3, 0, 3, 3, 1, 1, 3, 1, 0, 0, 0, 3, 3, 2, 3, 3, 3, 2, 2, 0, 1, 3, 1, 1, 1, 1, 2, 0, 2, 0, 2, 2, 2, 1, 0, 0, 2, 2, 2, 1, 3, 1, 2, 3, 0, 3, 3, 0, 3, 1, 3, 0, 3, 1, 3, 1, 2, 3, 1, 3, 0, 3, 3, 0, 2, 2, 2, 0, 2, 1, 0, 0, 1, 3, 3, 2, 3, 2, 1, 1, 3, 1, 2, 3, 2, 0, 2, 2, 0, 2, 3, 2, 1, 2, 0, 1, 1, 0, 1, 3, 2, 3, 1, 1, 1, 0, 1, 0, 3, 1, 2, 3, 2, 2, 1, 2, 2, 1, 2, 3, 3, 2, 0, 1, 1, 3, 1, 3, 0, 2, 1, 3, 2, 3, 3, 2, 1, 1, 3, 3, 2, 3, 2, 1, 3, 2, 3, 0, 0, 1, 2, 2, 1, 2, 1, 1, 3, 1, 3, 3, 3, 1, 3, 2, 2, 1, 3, 3, 3, 3, 3, 2, 2, 0, 2, 0, 2, 0, 2, 2, 3, 3, 3, 3, 3, 0, 0, 2, 1, 3, 0, 1, 2, 1, 2, 3, 0, 3, 1, 2, 0, 0, 3, 0, 0, 3, 2, 1, 1, 2, 2, 0, 3, 2, 1, 0, 3, 0, 2, 1, 2, 3, 1, 2, 1, 3, 3, 1, 1, 1, 1, 3, 3, 0, 1, 1, 2, 2, 0, 0, 0, 1, 0, 2, 2, 0, 3, 3, 3, 1, 1, 2, 3, 3, 0, 0, 2, 3, 0, 3, 2, 2, 0, 0, 1, 3, 0, 3, 0, 2, 2, 3, 0, 0, 1, 2, 0, 3, 3, 3, 3, 1, 0, 1, 0, 0, 3, 2, 0, 3, 3, 2, 2, 1, 2, 0, 0, 2, 2, 3, 2, 2, 1, 2, 3, 3, 0, 1, 3, 2, 0, 2, 1, 1, 0, 3, 2, 2, 3, 3, 2, 3, 2, 2, 0, 3, 0, 2, 1, 2, 0, 1, 2, 2, 2, 0, 2, 3, 1, 1, 2, 3, 2, 0, 3, 2, 1, 2, 1, 3, 3, 2, 0, 0, 2, 2, 2, 3, 1, 3, 0, 3, 3, 3, 1, 1, 2, 2, 0, 1, 1, 1, 3, 0, 1, 2, 2, 3, 2, 2, 3, 1, 2, 2, 2, 3, 3, 2, 2, 2, 3, 1, 2, 0, 3, 0, 3, 1, 3, 3, 2, 2, 1, 0, 1, 3, 3, 2, 0, 1, 1, 2, 0, 1, 1, 0, 1, 3, 2, 1, 0, 1, 2, 3, 0, 3, 1, 2, 3, 3, 1, 0, 1, 3, 2, 0, 0, 2, 3, 2, 1, 3, 0, 1, 1, 2, 1, 0, 0, 1, 0, 0, 3, 2, 0, 1, 2, 3, 2, 0, 3, 3, 1, 3, 1, 0, 2, 2, 3, 0, 1, 1, 3, 3, 3, 1, 0, 1, 3, 3, 1, 3, 0, 1, 1, 3, 1, 2, 3, 1, 3, 1, 3, 3, 2, 2, 1, 0, 1, 2, 2, 2, 1, 0, 0, 1, 0, 3, 3, 0, 1, 2, 2, 3, 3, 2, 0, 3, 2, 1, 3, 2, 1, 2, 3, 1, 1, 1, 2, 2, 0, 2, 1, 2, 0, 2, 1, 1, 0, 3, 2, 0, 0, 2, 2, 1, 3, 3, 3, 3, 3, 2, 0, 0, 1, 1, 1, 2, 2, 2, 1, 3, 1, 2, 0, 3, 2, 3, 1, 0, 3, 0, 1, 1, 0, 2, 3, 1, 0, 0, 1, 0, 1, 1, 3, 1, 3, 3, 1, 3, 2, 2, 2, 3, 3, 3, 1, 3, 0, 2, 0, 1, 2, 1, 1, 2, 2, 3, 2, 0, 2, 2, 3, 0, 3, 3, 0, 1, 3, 2, 3, 2, 1, 3, 2, 2, 3, 2, 1, 2, 3, 0, 0, 2, 2, 1, 3, 3, 2, 2, 0, 1, 3, 3, 3, 0, 2, 0, 3, 0, 3, 3, 1, 3, 0, 3, 2, 2, 1, 3, 1, 2, 0, 3, 1, 1, 3, 0, 3, 2, 3, 2, 3, 1, 1, 3, 2, 3, 0, 3, 3, 0, 3, 2, 2, 0, 0, 1, 3, 1, 2, 2, 1, 0, 3, 0, 0, 3, 2, 3, 2, 1, 1, 1, 2, 3, 2, 3, 2, 1, 3, 1, 1, 0, 2, 2, 1, 1, 1, 2, 0, 3, 3, 2, 3, 2, 3, 1, 1, 2, 2, 2, 2, 0, 3, 2, 0, 3, 0, 3, 0, 2, 1, 0, 1, 1, 1, 2, 0, 1, 3, 0, 1, 0, 2, 1, 0, 1, 1, 2, 3, 1, 3, 2, 3, 2, 0, 1, 1, 2, 3, 2, 3, 1, 3, 2, 2, 1, 0, 1, 3, 2, 2, 0, 1, 2, 3, 2, 0, 2, 1, 1, 2, 2, 0, 2, 3, 3, 2, 1, 2, 1, 0, 2, 1, 3, 3, 2, 1, 3, 2, 2, 0, 2, 2, 3, 1, 2, 2, 3, 2, 1, 3, 2, 0, 3, 0, 3, 1, 2, 1, 3, 2, 2, 1, 0, 3, 3, 0, 3, 1, 2, 1, 1, 2, 0, 0, 2, 2, 0, 2, 3, 3, 2, 0, 3, 3, 0, 3, 3, 0, 1, 3, 0, 1, 3, 2, 1, 3, 2, 3, 0, 3, 2, 3, 2, 3, 3, 1, 2, 0, 2, 1, 2, 1, 0, 0, 3, 0, 3, 0, 2, 2, 3, 0, 3, 3, 3, 2, 2, 2, 1, 2, 0, 3, 3, 1, 0, 3, 2, 3, 3, 3, 1, 3, 3, 3, 0, 3, 2, 0, 0, 1, 1, 3, 2, 1, 1, 0, 1, 3, 2, 0, 0, 1, 1, 2, 2, 0, 0, 0, 1, 1, 3, 0, 3, 0, 2, 1, 3, 3, 1, 3, 1, 2, 0, 3, 0, 3, 2, 2, 1, 2, 0, 2, 1, 1, 3, 0, 3, 3, 1, 1, 2, 3, 0, 2, 2, 2, 0, 1, 2, 2, 1, 3, 1, 1, 3, 3, 0, 1, 0, 2, 0, 1, 2, 0, 0, 3, 1, 2, 3, 1, 2, 0, 1, 0, 2, 0, 3, 1, 0, 2, 3, 0, 1, 2, 3, 0, 0, 3, 3, 2, 0, 3, 1, 3, 1, 3, 2, 3, 2, 1, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 0, 2, 0, 3, 1, 3, 2, 0, 1, 2, 2, 0, 1, 3, 0, 1, 1, 3, 2, 2, 3, 2, 1, 2, 2, 3, 3, 1, 1, 2, 0, 3, 1, 0, 3, 3, 0, 3, 3, 2, 0, 2, 1, 1, 0, 2, 2, 0, 2, 1, 0, 0, 0, 3, 2, 1, 3, 0, 2, 3, 0, 1, 1, 3, 3, 3, 2, 0, 2, 3, 1, 3, 1, 0, 1, 0, 1, 2, 2, 3, 2, 2, 2, 1, 3, 2, 0, 0, 3, 1, 0, 0, 2, 3, 1, 0, 3, 2, 2, 2, 1, 3, 2, 2, 1, 3, 3, 3, 1, 0, 0, 2, 3, 1, 2, 0, 0, 2, 2, 3, 3, 1, 1, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 1, 1, 2, 3, 3, 1, 2, 3, 3, 2, 2, 0, 3, 2, 2, 3, 1, 3, 1, 0, 0, 3, 3, 3, 3, 2, 3, 3, 3, 1, 2, 1, 0, 1, 1, 0, 0, 2, 2, 0, 2, 1, 3, 0, 3, 0, 3, 2, 1, 3, 3, 1, 3, 2, 0, 2, 2, 3, 1, 2, 0, 3, 0, 2, 3, 1, 1, 0, 0, 2, 3, 0, 3, 0, 1, 2, 2, 0, 3, 0, 2, 1, 2, 2, 3, 1, 0, 1, 2, 2, 1, 3, 3, 1, 0, 1, 1, 0, 1, 3, 1, 2, 0, 2, 3, 2, 3, 2, 0, 2, 3, 3, 1, 1, 0, 3, 1, 3, 3, 0, 1, 1, 1, 3, 3, 2, 2, 3, 2, 0, 0, 1, 2, 3, 3, 1, 2, 2, 2, 2, 2, 3, 0, 3, 2, 1, 1, 3, 2, 2, 0, 0, 1, 2, 1, 1, 2, 1, 3, 0, 0, 1, 3, 0, 0, 2, 2, 3, 2, 0, 3, 2, 1, 0, 3, 1, 0, 1, 3, 3, 0, 1, 2, 0, 1, 0, 1, 2, 0, 1, 1, 0, 3, 2, 0, 0, 3, 0, 3, 0, 1, 3, 0, 0, 2, 2, 0, 2, 2, 2, 3, 2, 1, 2, 2, 3, 0, 1, 2, 1, 2, 3, 2, 3, 2, 0, 3, 0, 2, 1, 0, 0, 0, 0, 2, 3, 1, 3, 3, 1, 0, 2, 1, 2, 2, 3, 0, 0, 0, 3, 1, 3, 1, 0, 0, 1, 2, 0, 2, 3, 3, 0, 2, 1, 2, 2, 3, 3, 2, 3, 3, 2, 3, 0, 0, 2, 3, 0, 1, 2, 3, 2, 2, 1, 0, 1, 0, 2, 3, 1, 2, 1, 3, 3, 2, 2, 3, 3, 1, 1, 0, 3, 3, 1, 1, 3, 2, 0, 0, 3, 0, 2, 2, 2, 3, 0, 0, 2, 0, 1, 3, 0, 2, 2, 2, 2, 2, 3, 3, 3, 0, 2, 0, 2, 1, 0, 3, 3, 0, 1, 2, 0, 3, 0, 2, 1, 2, 0, 3, 2, 3, 2, 1, 2, 0, 3, 3, 1, 1, 0, 0, 0, 1, 2, 2, 2, 3, 2, 2, 1, 3, 0, 2, 2, 1, 1, 2, 0, 3, 1, 0, 2, 2, 1, 3, 2, 2, 2, 1, 1, 0, 0, 0, 3, 0, 3, 1, 2, 2, 2, 0, 0, 1, 3, 2, 1, 0, 1, 0, 3, 2, 0, 2, 3, 1, 2, 1, 0, 1, 2, 0, 3, 0, 3, 3, 2, 1, 3, 2, 1, 2, 3, 2, 0, 2, 0, 1, 3, 2, 2, 1, 0, 3, 1, 1, 3, 2, 0, 1, 3, 0, 2, 3, 2, 2, 1, 1, 1, 1, 2, 3, 2, 1, 0, 2, 2, 3, 0, 1, 2, 3, 3, 1, 2, 2, 1, 0, 3, 3, 2, 3, 0, 1, 2, 3, 2, 3, 2, 0, 2, 2, 2, 2, 2, 1, 0, 2, 3, 1, 3, 3, 2, 0, 2, 0, 3, 1, 1, 1, 2, 1, 3, 3, 0, 3, 3, 2, 0, 0, 1, 3, 1, 0, 1, 2, 2, 3, 1, 1, 1, 3, 3, 2, 3, 1, 2, 2, 0, 2, 3, 3, 2, 3, 3, 3, 1, 3, 1, 3, 2, 0, 3, 3, 2, 1, 2, 1, 2, 1, 3, 3, 2, 1, 3, 0, 3, 2, 2, 1, 3, 3, 2, 2, 2, 2, 2, 1, 2, 3, 2, 2, 2, 3, 2, 1, 2, 2, 3, 3, 0, 3, 2, 0, 0, 3, 3, 0, 0, 0, 2, 1, 1, 2, 2, 0, 1, 3, 1, 2, 2, 1, 2, 2, 1, 0, 2, 3, 1, 3, 3, 2, 2, 2, 3, 0, 1, 3, 1, 2, 0, 0, 1, 0, 0, 1, 0, 3, 1, 0, 1, 2, 2, 1, 1, 3, 1, 1, 3, 1, 3, 2, 1, 2, 0, 1, 2, 0, 0, 1, 3, 1, 0, 3, 1, 0, 0, 0, 2, 1, 1, 2, 2, 1, 0, 3, 2, 2, 3, 3, 2, 0, 0, 2, 2, 2, 2, 2, 3, 0, 0, 0, 2, 1, 2, 1, 0, 0, 0, 1, 1, 2, 1, 0, 1, 0, 1, 2, 3, 3, 2, 1, 2, 3, 3, 1, 1, 0, 1, 3, 3, 1, 3, 1, 3, 2, 2, 0, 1, 3, 2, 0, 2, 3, 1, 3, 0, 3, 2, 1, 2, 0, 1, 3, 2, 0, 1, 1, 0, 1, 2, 1, 2, 2, 3, 0, 1, 1, 1, 0, 2, 0, 0, 0, 1, 3, 0, 1, 2, 1, 2, 3, 2, 1, 0, 1, 3, 3, 0, 2, 1, 0, 0, 3, 1, 0, 1, 3, 0, 1, 2, 3, 3, 0, 2, 3, 2, 1, 0, 3, 2, 0, 0, 3, 1, 3, 0, 3, 2, 3, 3, 3, 1, 2, 0, 3, 1, 0, 0, 2, 2, 0, 3, 3, 2, 0, 1, 3, 0, 1, 3, 0, 1, 0, 1, 0, 3, 2, 0, 3, 2, 1, 0, 2, 0, 0, 2, 3, 3, 1, 2, 3, 0, 2, 0, 1, 0, 3, 2, 1, 0, 2, 2, 3, 0, 3, 1, 3, 2, 1, 3, 0, 2, 0, 1, 0, 2, 1, 2, 2, 3, 1, 2, 1, 1, 3, 2, 0, 2, 3, 0, 1, 3, 3, 3, 2, 1, 3, 1, 3, 3, 3, 3, 3, 1, 0, 1, 3, 1, 1, 1, 3, 1, 0, 3, 0, 1, 2, 1, 0, 3, 3, 1, 2, 1, 3, 1, 3, 1, 2, 2, 0, 2, 1, 3, 0, 2, 3, 3, 2, 0, 2, 2, 3, 2, 0, 3, 2, 1, 1, 0, 1, 3, 1, 3, 1, 0, 2, 3, 2, 2, 3, 0, 1, 0, 3, 1, 0, 2, 2, 3, 1, 2, 0, 2, 3, 2, 3, 3, 1, 0, 1, 2, 1, 3, 1, 2, 3, 0, 2, 1, 0, 1, 3, 2, 2, 1, 3, 2, 0, 1, 2, 1, 0, 1, 0, 2, 3, 2, 1, 2, 3, 2, 0, 2, 1, 2, 3, 3, 2, 1, 3, 1, 3, 3, 2, 3, 1, 0, 3, 1, 3, 1, 1, 1, 1, 0, 0, 3, 0, 2, 3, 2, 2, 2, 0, 3, 3, 2, 1, 0, 0, 1, 2, 1, 2, 2, 0, 2, 1, 0, 2, 1, 0, 3, 0, 1, 3, 0, 3, 3, 2, 1, 1, 1, 0, 1, 2, 3, 3, 2, 2, 0, 0, 1, 2, 0, 2, 3, 2, 0, 3, 2, 1, 2, 1, 0, 0, 0, 2, 1, 0, 3, 2, 2, 3, 2, 3, 1, 2, 3, 3, 0, 2, 2, 1, 1, 2, 2, 3, 1, 2, 0, 2, 0, 1, 2, 2, 2, 2, 3, 3, 0, 2, 0, 0, 3, 1, 3, 3, 0, 2, 1, 1, 1, 3, 1, 3, 3, 0, 0, 2, 1, 0, 2, 1, 0, 1, 2, 1, 0, 0, 2, 3, 2, 3, 3, 2, 2, 1, 2, 1, 1, 3, 3, 1, 2, 2, 2, 3, 3, 2, 2, 1, 1, 3, 2, 2, 1, 3, 3, 1, 2, 2, 3, 3, 1, 3, 3, 3, 3, 1, 2, 3, 3, 1, 1, 2, 3, 1, 3, 3, 2, 1, 1, 0, 3, 3, 3, 3, 1, 0, 3, 3, 2, 2, 2, 3, 0, 2, 2, 1, 2, 2, 2, 0, 1, 3, 2, 0, 2, 3, 0, 2, 3, 0, 0, 3, 0, 3, 3, 2, 3, 2, 2, 1, 0, 2, 3, 2, 0, 1, 0, 1, 1, 0, 2, 3, 1, 2, 2, 3, 2, 2, 0, 0, 2, 0, 2, 0, 1, 1, 3, 1, 3, 3, 2, 3, 1, 0, 2, 2, 0, 3, 3, 1, 3, 0, 2, 3, 1, 1, 2, 1, 3, 0, 1, 3, 1, 2, 3, 0, 2, 3, 1, 2, 2, 3, 0, 0, 2, 3, 2, 0, 3, 2, 0, 0, 3, 2, 0, 3, 3, 3, 1, 2, 3, 2, 3, 0, 3, 2, 2, 3, 1, 1, 0, 2, 3, 0, 2, 0, 0, 1, 2, 1, 1, 1, 3, 2, 0, 2, 2, 0, 2, 1, 2, 3, 3, 3, 2, 3, 3, 1, 1, 0, 1, 0, 0, 3, 3, 3, 1, 1, 1, 3, 3, 2, 0, 1, 3, 3, 2, 0, 3, 1, 0, 0, 3, 1, 1, 0, 3, 2, 0, 1, 3, 1, 0, 1, 1, 0, 1, 3, 2, 0, 2, 0, 2, 3, 0, 3, 0, 3, 3, 3, 0, 2, 3, 0, 2, 1, 3, 2, 2, 1, 1, 2, 3, 1, 2, 1, 2, 3, 1, 1, 2, 2, 0, 0, 0, 1, 3, 2, 0, 1, 2, 0, 2, 0, 2, 1, 0, 2, 3, 0, 1, 2, 3, 3, 1, 0, 0, 0, 1, 3, 2, 2, 0, 1, 1, 2, 0, 3, 0, 1, 1, 2, 0, 1, 3, 0, 3, 1, 0, 1, 2, 0, 3, 1, 0, 0, 0, 2, 3, 0, 3, 2, 3, 2, 3, 1, 1, 3, 2, 3, 2, 0, 0, 1, 2, 3, 3, 1, 0, 1, 1, 0, 1, 3, 3, 3, 1, 2, 0, 1, 2, 1, 0, 2, 2, 0, 3, 2, 3, 0, 3, 1, 2, 0, 0, 2, 2, 0, 2, 3, 1, 1, 3, 0, 1, 1, 1, 3, 3, 0, 3, 1, 2, 2, 1, 2, 3, 3, 1, 0, 2, 3, 3, 0, 0, 0, 1, 3, 3, 0, 3, 3, 0, 3, 2, 3, 3, 0, 3, 3, 3, 2, 0, 3, 1, 1, 1, 3, 1, 0, 2, 3, 3, 0, 1, 3, 0, 2, 2, 3, 1, 2, 1, 3, 3, 0, 0, 1, 0, 2, 2, 2, 1, 2, 0, 0, 1, 2, 0, 2, 1, 1, 0, 1, 1, 2, 1, 0, 3, 1, 1, 2, 1, 0, 3, 1, 1, 3, 1, 1, 2, 0, 3, 2, 1, 0, 0, 2, 2, 0, 2, 1, 3, 1, 1, 2, 2, 2, 1, 2, 0, 3, 0, 3, 3, 2, 3, 3, 2, 2, 2, 0, 0, 1, 1, 0, 3, 0, 1, 3, 1, 3, 3, 2, 2, 2, 1, 3, 2, 2, 3, 0, 2, 0, 0, 1, 3, 2, 0, 2, 0, 1, 0, 2, 1, 3, 1, 0, 3, 3, 0, 3, 3, 1, 0, 3, 3, 1, 0, 0, 0, 3, 3, 1, 2, 3, 0, 3, 0, 3, 2, 1, 0, 2, 3, 3, 3, 3, 0, 3, 2, 0, 2, 0, 1, 3, 1, 0, 3, 1, 1, 3, 1, 2, 3, 3, 2, 1, 2, 0, 1, 2, 1, 2, 2, 2, 2, 3, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 2, 3, 1, 1, 0, 3, 0, 0, 2, 3, 0, 1, 1, 1, 2, 0, 0, 3, 1, 2, 1, 2, 0, 3, 3, 1, 2, 2, 0, 0, 3, 2, 0, 1, 3, 3, 0, 2, 2, 2, 3, 1, 2, 1, 2, 2, 0, 3, 0, 2, 0, 0, 2, 3, 3, 0, 1, 3, 2, 1, 3, 1, 2, 2, 3, 3, 0, 1, 0, 1, 3, 3, 0, 1, 1, 1, 1, 0, 1, 2, 3, 0, 1, 2, 1, 2, 1, 0, 3, 0, 0, 1, 3, 2, 3, 3, 1, 0, 1, 1, 0, 1, 2, 1, 3, 1, 1, 3, 2, 3, 3, 2, 2, 1, 0, 3, 0, 2, 1, 1, 3, 2, 0, 0, 1, 3, 3, 3, 2, 1, 3, 1, 2, 0, 2, 0, 0, 0, 2, 1, 1, 0, 3, 1, 3, 0, 0, 1, 2, 1, 0, 2, 3, 2, 1, 0, 3, 3, 2, 1, 1, 0, 1, 3, 0, 2, 2, 2, 2, 1, 3, 3, 3, 0, 3, 2, 2, 1, 1, 0, 3, 2, 3, 2, 2, 1, 1, 3, 2, 1, 2, 2, 2, 1, 0, 3, 2, 1, 3, 3, 1, 1, 0, 3, 1, 2, 1, 2, 2, 2, 1, 1, 1, 0, 0, 1, 1, 1, 3, 1, 3, 1, 1, 2, 2, 0, 3, 0, 1, 1, 3, 1, 3, 0, 2, 2, 0, 0, 1, 3, 2, 3, 0, 0, 2, 3, 2, 1, 2, 3, 3, 3, 0, 3, 2, 1, 1, 2, 3, 2, 2, 1, 3, 0, 0, 3, 3, 0, 3, 0, 2, 3, 0, 0, 3, 0, 2, 1, 1, 3, 0, 0, 2, 1, 0, 2, 3, 3, 1, 1, 2, 2, 2, 1, 3, 3, 2, 0, 2, 1, 2, 0, 1, 2, 1, 1, 1, 3, 2, 0, 1, 1, 3, 2, 1, 1, 3, 3, 2, 3, 1, 1, 1, 1, 3, 3, 1, 1, 0, 2, 2, 3, 0, 0, 2, 0, 2, 3, 0, 1, 2, 1, 2, 1, 2, 0, 2, 2, 0, 0, 0, 0, 2, 1, 0, 0, 3, 3, 1, 3, 0, 2, 0, 2, 3, 3, 3, 2, 1, 2, 1, 1, 0, 3, 3, 2, 1, 3, 1, 0, 2, 0, 3, 0, 3, 0, 3, 2, 0, 1, 0, 3, 2, 2, 0, 1, 2, 2, 0, 3, 1, 3, 0, 0, 2, 3, 1, 2, 2, 2, 0, 1, 0, 0, 2, 3, 0, 1, 0, 1, 0, 3, 0, 3, 2, 2, 0, 0, 3, 2, 3, 1, 2, 0, 2, 1, 3, 2, 2, 1, 2, 1, 3, 3, 1, 2, 1, 1, 2, 3, 2, 0, 1, 0, 3, 0, 2, 2, 2, 1, 3, 0, 2, 1, 0, 1, 1, 2, 3, 3, 2, 0, 0, 1, 1, 0, 2, 0, 1, 3, 0, 2, 2, 0, 0, 1, 2, 1, 2, 2, 3, 3, 0, 3, 2, 1, 2, 3, 2, 3, 1, 1, 2, 3, 3, 2, 3, 0, 0, 1, 2, 2, 1, 0, 3, 3, 1, 1, 0, 3, 2, 0, 2, 3, 0, 0, 0, 3, 3, 2, 2, 3, 1, 0, 2, 0, 3, 2, 1, 0, 2, 0, 1, 3, 3, 3, 2, 1, 3, 1, 2, 1, 3, 2, 2, 2, 1, 1, 2, 1, 0, 3, 0, 2, 3, 1, 1, 2, 2, 0, 1, 3, 3, 2, 1, 3, 3, 0, 1, 2, 3, 2, 1, 0, 3, 3, 1, 2, 1, 0, 1, 3, 0, 1, 1, 0, 3, 0, 1, 2, 1, 3, 3, 1, 1, 2, 0, 0, 2, 1, 3, 2, 1, 1, 0, 3, 3, 1, 2, 2, 0, 3, 2, 1, 1, 1, 0, 3, 0, 2, 1, 1, 3, 0, 2, 1, 3, 2, 3, 3, 0, 0, 1, 0, 1, 2, 3, 3, 3, 1, 0, 3, 3, 3, 1, 2, 2, 2, 1, 3, 2, 3, 0, 3, 2, 1, 1, 0, 3, 1, 3, 1, 0, 3, 1, 3, 2, 3, 2, 3, 1, 3, 3, 0, 2, 3, 1, 3, 2, 3, 1, 1, 2, 3, 1, 0, 0, 0, 2, 0, 3, 3, 0, 0, 1, 2, 1, 1, 1, 0, 3, 2, 0, 2, 0, 3, 3, 0, 0, 1, 3, 3, 3, 3, 2, 1, 2, 2, 1, 3, 1, 3, 1, 2, 2, 1, 2, 3, 3, 0, 2, 2, 1, 2, 0, 3, 3, 0, 3, 2, 2, 0, 0, 2, 0, 1, 2, 3, 0, 1, 0, 3, 1, 0, 2, 0, 1, 3, 1, 2, 0, 1, 0, 2, 3, 3, 2, 3, 2, 1, 2, 2, 0, 0, 1, 2, 2, 0, 2, 3, 2, 2, 1, 0, 1, 2, 1, 2, 3, 2, 3, 3, 3, 1, 0, 1, 1, 1, 2, 2, 3, 2, 3, 1, 3, 3, 2, 2, 2, 1, 0, 2, 0, 0, 1, 1, 2, 2, 3, 2, 3, 1, 2, 3, 0, 1, 2, 2, 2, 0, 0, 3, 3, 1, 3, 1, 0, 1, 1, 1, 2, 1, 3, 2, 1, 0, 3, 3, 1, 1, 1, 3, 0, 0, 2, 3, 3, 1, 1, 2, 1, 3, 1, 1, 0, 0, 3, 1, 3, 3, 1, 1, 2, 2, 2, 0, 1, 1, 2, 3, 3, 1, 3, 0, 0, 3, 0, 2, 2, 3, 0, 0, 2, 0, 3, 1, 1, 1, 3, 1, 3, 0, 2, 1, 0, 3, 2, 3, 1, 2, 0, 1, 2, 0, 3, 1, 1, 2, 0, 2, 0, 2, 0, 0, 2, 1, 0, 2, 2, 1, 1, 1, 0, 2, 3, 0, 0, 2, 1, 3, 2, 0, 3, 2, 0, 2, 0, 0, 3, 1, 1, 0, 3, 0, 3, 2, 1, 0, 0, 0, 3, 0, 1, 0, 2, 2, 3, 3, 0, 3, 2, 0, 1, 3, 1, 3, 2, 3, 2, 2, 2, 0, 1, 3, 0, 0, 2, 2, 0, 1, 0, 1, 0, 0, 2, 0, 1, 1, 1, 2, 3, 0, 1, 1, 1, 3, 0, 3, 3, 1, 3, 2, 0, 2, 0, 2, 2, 1, 3, 2, 3, 0, 2, 3, 1, 0, 2, 1, 3, 0, 2, 2, 2, 1, 1, 1, 2, 3, 2, 2, 1, 0, 1, 0, 1, 1, 2, 0, 1, 1, 3, 2, 1, 1, 2, 2, 3, 0, 0, 2, 1, 0, 0, 3, 3, 2, 0, 3, 1, 0, 2, 0, 0, 0, 3, 1, 2, 2, 1, 1, 2, 3, 3, 1, 0, 2, 2, 0, 2, 1, 1, 3, 0, 0, 1, 2, 0, 1, 1, 2, 2, 0, 1, 2, 0, 2, 0, 1, 2, 1, 0, 1, 2, 0, 2, 3, 1, 1, 0, 3, 3, 2, 0, 1, 0, 0, 1, 3, 0, 2, 2, 2, 0, 0, 3, 1, 1, 0, 1, 2, 3, 0, 3, 1, 0, 1, 3, 2, 3, 0, 3, 1, 2, 2, 3, 1, 1, 0, 1, 1, 0, 0, 3, 1, 0, 1, 3, 0, 0, 0, 1, 0, 2, 2, 1, 2, 1, 2, 1, 2, 3, 0, 2, 2, 0, 1, 1, 3, 2, 2, 3, 3, 2, 1, 1, 3, 1, 0, 2, 1, 1, 0, 1, 0, 2, 3, 0, 0, 3, 3, 1, 0, 0, 0, 0, 3, 1, 2, 0, 2, 2, 3, 1, 0, 0, 1, 3, 1, 3, 2, 0, 3, 3, 3, 0, 2, 2, 3, 0, 2, 1, 3, 2, 0, 0, 3, 2, 0, 0, 0, 3, 0, 2, 1, 3, 1, 0, 3, 0, 0, 0, 2, 1, 2, 1, 0, 0, 2, 2, 0, 0, 1, 2, 2, 3, 2, 3, 3, 3, 0, 1, 0, 2, 1, 2, 2, 2, 2, 0, 2, 2, 1, 1, 0, 1, 3, 1, 1, 2, 3, 1, 0, 0, 1, 1, 1, 0, 2, 2, 1, 0, 1, 2, 1, 3, 0, 0, 0, 1, 0, 1, 1, 1, 2, 2, 2, 1, 1, 0, 3, 3, 0, 1, 1, 1, 2, 0, 3, 2, 1, 3, 3, 2, 1, 3, 0, 1, 3, 0, 1, 2, 3, 0, 0, 3, 1, 1, 0, 3, 2, 2, 0, 1, 2, 3, 3, 3, 0, 0, 2, 3, 1, 3, 2, 0, 0, 1, 2, 0, 1, 1, 2, 0, 0, 0, 2, 1, 3, 1, 0, 0, 3, 0, 0, 3, 3, 0, 3, 1, 1, 1, 3, 1, 1, 2, 1, 3, 3, 1, 3, 0, 2, 3, 0, 2, 2, 1, 2, 2, 3, 3, 2, 2, 1, 3, 3, 2, 2, 2, 2, 2, 1, 1, 2, 3, 1, 1, 3, 3, 0, 1, 3, 1, 1, 0, 2, 1, 1, 0, 1, 0, 0, 1, 2, 0, 2, 0, 3, 2, 2, 3, 1, 1, 3, 1, 2, 0, 2, 2, 2, 2, 3, 0, 0, 1, 2, 3, 3, 1, 2, 2, 2, 1, 3, 0, 0, 0, 1, 3, 3, 1, 1, 2, 3, 0, 3, 0, 3, 1, 1, 1, 0, 0, 1, 1, 0, 2, 1, 3, 3, 2, 3, 3, 2, 1, 1, 2, 2, 0, 0, 1, 1, 3, 1, 3, 1, 3, 3, 2, 1, 1, 3, 2, 2, 2, 3, 3, 0, 0, 2, 1, 2, 3, 1, 3, 3, 2, 2, 1, 1, 2, 3, 2, 0, 3, 0, 1, 3, 3, 2, 1, 3, 2, 0, 1, 2, 2, 1, 3, 3, 1, 2, 2, 3, 0, 3, 3, 1, 1, 2, 1, 1, 2, 2, 0, 2, 1, 0, 2, 1, 0, 3, 0, 2, 1, 2, 3, 2, 1, 0, 1, 3, 3, 3, 0, 1, 0, 3, 2, 1, 2, 2, 0, 2, 3, 1, 0, 2, 0, 0, 2, 0, 3, 0, 0, 0, 2, 2, 3, 2, 2, 1, 0, 0, 0, 2, 1, 1, 2, 3, 3, 3, 0, 1, 3, 2, 3, 0, 3, 0, 0, 3, 2, 2, 3, 2, 3, 1, 2, 0, 2, 0, 2, 3, 0, 3, 2, 1, 2, 1, 2, 2, 1, 2, 2, 0, 1, 3, 3, 0, 2, 3, 1, 0, 2, 1, 2, 3, 1, 2, 0, 1, 0, 3, 0, 2, 1, 2, 0, 3, 2, 3, 1, 0, 0, 1, 3, 3, 1, 0, 2, 3, 0, 1, 0, 2, 1, 3, 3, 2, 3, 2, 3, 0, 3, 3, 2, 2, 0, 3, 3, 1, 1, 0, 0, 0, 1, 2, 3, 2, 2, 1, 3, 2, 3, 1, 2, 3, 1, 0, 3, 3, 3, 2, 1, 1, 0, 2, 2, 2, 3, 0, 2, 3, 0, 3, 3, 2, 3, 2, 3, 3, 0, 1, 1, 0, 2, 3, 0, 3, 0, 2, 2, 3, 2, 2, 0, 3, 0, 2, 1, 1, 3, 0, 3, 2, 0, 3, 2, 0, 0, 2, 2, 0, 3, 0, 3, 2, 2, 1, 3, 2, 3, 2, 0, 0, 2, 3, 0, 3, 2, 3, 2, 3, 0, 3, 2, 1, 3, 0, 1, 0, 1, 0, 0, 3, 2, 2, 0, 1, 2, 0, 0, 2, 0, 1, 0, 1, 3, 0, 3]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
